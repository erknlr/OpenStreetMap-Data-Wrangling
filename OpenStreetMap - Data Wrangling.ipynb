{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to brush up my data wrangling skills I would like to work with OpenStreetMap data for **San Francisco**. As a technology enthusiast, this dataset is the perfect opportunity for me to get somewhat familiar with the city. I will first use python to clean up/structure data and then use **NoSQL/MongoDB** to store it and run queries on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be taking the following steps:\n",
    "\n",
    "- Getting a general idea about the file.\n",
    "- Auditing for flaws and coming up with cleaning approaches.\n",
    "- Reshaping the data into an easy to work with python dictionary format.\n",
    "- Converting the new dictionary format to JSON.\n",
    "- Importing the JSON file to MongoDB.\n",
    "- Running queries and discovering the dataset.\n",
    "- Coming up with additional ideas about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##First thing first, let's import all of the libraries that will be necessary in our wrangling process: \n",
    "\n",
    "import os\n",
    "import pymongo\n",
    "import xml.etree.cElementTree as ET\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import bson\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import os.path\n",
    "import sys\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a general idea about the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for San Francisco can be obtained here:\n",
    " \n",
    "https://mapzen.com/data/metro-extracts/metro/san-francisco_california/\n",
    "\n",
    "The zipped bz2 file is **87.1 MB** and the unzipped OSM file is **1.4 GB** big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into auditing, it is a good idea to get a general impression about the data that we have. So let's first start by investigating the different tags and their number of occurences in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_num_tags(file_name):\n",
    "        num_tags = {}\n",
    "        for event, elem in ET.iterparse(file_name):\n",
    "            if elem.tag in num_tags:\n",
    "                num_tags[elem.tag] +=1\n",
    "            else:\n",
    "                num_tags[elem.tag] = 1\n",
    "        return sorted(num_tags.items(), key = lambda x: x[1], reverse=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nd', 7768878),\n",
      " ('node', 6554636),\n",
      " ('tag', 2120322),\n",
      " ('way', 806770),\n",
      " ('member', 54869),\n",
      " ('relation', 6211),\n",
      " ('bounds', 1),\n",
      " ('osm', 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(count_num_tags('san-francisco_california.osm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going one step further, I would like to also see the 5 most frequent occurences of the tag keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_tag_keys(file_name):\n",
    "        num_keys = {}\n",
    "        for event, elem in ET.iterparse(file_name):\n",
    "            for attr in elem.iter(\"tag\"):\n",
    "                key = attr.attrib[\"k\"]\n",
    "\n",
    "                if key not in num_keys:\n",
    "                    num_keys[key] = 1\n",
    "                else:\n",
    "                    num_keys[key] += 1\n",
    "\n",
    "        return sorted(num_keys.items(), key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('building', 2022663),\n",
      " ('height', 430539),\n",
      " ('highway', 385911),\n",
      " ('name', 270051),\n",
      " ('addr:housenumber', 191157)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(audit_tag_keys('san-francisco_california.osm')[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing for flaws and coming up with cleaning approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that I would like to audit (and correct if neccesary) are the address indicators like *\"Street\", \"Avenue\"*. Initially, I am interested in all non-usual indicators besides:\n",
    "\n",
    "- Street\n",
    "- Avenue\n",
    "- Boulevard\n",
    "- Drive\n",
    "- Court\n",
    "- Place\n",
    "- Square\n",
    "- Lane\n",
    "- Road\n",
    "- Trail\n",
    "- Parkway\n",
    "- Commons\n",
    "- Way\n",
    "- Highway\n",
    "- Circle\n",
    "- Terrace\n",
    "- Alley\n",
    "- Plaza\n",
    "- Braodway\n",
    "- Bridgeway\n",
    "- Mall\n",
    "- Walk\n",
    "- Center\n",
    "- Park\n",
    "- Route\n",
    "- View\n",
    "- Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I will use the following regex to catch the word endings:\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# These are my standard list of indicators. I am interested in values besides these.\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Way\", \"Highway\", \"Circle\", \"Terrace\", \"Alley\", \"Plaza\", \"Broadway\", \n",
    "            \"Bridgeway\", \"Mall\", \"Walk\", \"Center\", \"Park\", \"Route\", \"Pier\", \"View\", \"Bridge\"]\n",
    "\n",
    "# Function to catch unusual indicators\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "            \n",
    "# Function to look only for addr:street keys             \n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "# Main function to combine the 2 functions above and create a list of unique values that are not in \"expected\" \n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a sample of the map and take a look at which words are not matching our expected group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'105': set(['Grand Avenue #105']),\n",
      " 'Alameda': set(['Alameda']),\n",
      " 'Ave': set(['Tehama Ave']),\n",
      " 'D': set(['Avenue D']),\n",
      " 'Gardens': set(['Wildwood Gardens']),\n",
      " 'Real': set(['El Camino Real']),\n",
      " 'St': set(['Kearny St'])}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dict(audit('sample_san_francisco_100.osm')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, we have to address the following cases:\n",
    "\n",
    "- **Abbreviations**: They need to be replaced with the full version. (eg. \"Blvd\")\n",
    "- **Typos**: They need to be corrected. (eg. \"Avenie\")\n",
    "- **Capital/Non-Capital Letters**: They need to be standardized. (eg. \"way\")\n",
    "- ** Additional Characters**: Some indicators have additional letters & numbers at the end. Those could be either shortened or expanded. (eg. \"Grand Avenue #105\" => \"Grand Avenue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our mapping for corrections:\n",
    "mapping = { \n",
    "            \"ave.\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"AVE\": \"Avenue\",\n",
    "            \"avenue\": \"Avenue\",\n",
    "            \"Avenie\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd,\": \"Boulevard\",\n",
    "            \"blvd\": \"Boulevard\",\n",
    "            \"Ctr\": \"Center\",\n",
    "            \"dr\": \"Drive\",\n",
    "            \"Dr\": \"Drive\",    \n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"rd\": \"Road\",\n",
    "            \"rd.\": \"Road\",\n",
    "            \"Rd.\": \"Road\",       \n",
    "            \"ln\": \"Lane\",\n",
    "            \"ln.\": \"Lane\",\n",
    "            \"Ln.\": \"Lane\",\n",
    "            \"st\": \"Street\",\n",
    "            \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"square\": \"Square\",\n",
    "            \"plz\": \"Plaza\",\n",
    "            \"Pl\": \"Plaza\",\n",
    "            \"Plz\": \"Plaza\",\n",
    "            \"way\": \"Way\",\n",
    "            \"Woodside Road, Suite 100\": \"Woodside Road\",\n",
    "            \"Grand Avenue #105\": \"Grand Avenue\",\n",
    "            \"N California Blvd\": \"N California Boulevard\",    \n",
    "            \"Doolittle Dr, Suite 15\": \"Doolittle Drive\",    \n",
    "            \"Brannan St #151\": \"Brannan Street\",    \n",
    "            \"University Ave #155\": \"University Avenue\",    \n",
    "            \"Woodside Road, Suite 155\": \"Woodside Road\",\n",
    "            \"San Francisco Bicycle Route 2\": \"San Francisco Bicycle Route\",\n",
    "            \"Bartlett Street #203\": \"Bartlett Street\",\n",
    "            \"University Dr. Suite 3\": \"University Drive\",\n",
    "            \"Mission Blvd #302\": \"Mission Boulevard\",\n",
    "            \"Sansome St #3500\": \"Sansome Street\",\n",
    "            \"Market Street Suite 3658\": \"Market Street\",\n",
    "            \"Pier 39\": \"Pier 39 Pier\",\n",
    "            \"Menlo Ave  # 4\": \"Menlo Avenue\",\n",
    "            \"16th St #404\": \"16th Street\",\n",
    "            \"Sansome Street Ste 730\": \"Sansome Street\",\n",
    "            \"Sansome Street Suite 730\": \"Sansome Street\",\n",
    "            \"Pier 50 A\": \"Pier 50 A Pier\",\n",
    "            \"San Francisco Internation Airport\": \"San Francisco International Airport\",\n",
    "            \"Avenue B\": \"Avenue B Avenue\",\n",
    "            \"Pier 50 B\": \"Pier 50 B Pier\",\n",
    "            \"Avenue D\": \"Avenue D Avenue\",\n",
    "            \"Marina Boulevard Building D\": \"Marina Boulevard Building D Boulevard\",\n",
    "            \"Avenue E\": \"Avenue E Avenue\",\n",
    "            \"Francisco Boulevard East\": \"Francisco Boulevard East Boulevard\",\n",
    "            \"Sir Francis Drake Boulevard East\": \"Sir Francis Drake Boulevard East Boulevard\",\n",
    "            \"Avenue F\": \"Avenue F Avenue\",\n",
    "            \"Avenue G\": \"Avenue G Avenue\",\n",
    "            \"Ne Quad I-280 / Edgewood Rd Ic\": \"Ne Quad I-280 / Edgewood Road\",\n",
    "            \"Nw Quad Sr 84 / Ardenwood Blvd Ic\": \"Nw Quad Sr 84 / Ardenwood Blvd Ic Boulevard\",\n",
    "            \"Cabrillo Highway North\": \"Cabrillo Highway\",\n",
    "            \"Mission Bay Boulevard North\": \"Mission Bay Boulevard\",\n",
    "            \"Avenue Del Ora\": \"Avenue Del Ora Avenue\",\n",
    "            \"Buena Vista Avenue West\": \"Buena Vista Avenue West Avenue\"\n",
    "          }\n",
    "\n",
    "# Function to update the names\n",
    "def update_name(name, mapping):\n",
    "     if name in mapping:\n",
    "        return mapping[name]\n",
    "     else:  \n",
    "        name_list = re.findall(r\"[\\w']+\", name)\n",
    "        end_of_street_name = len(name_list)  \n",
    "        for i in range(len(name_list)):\n",
    "            word = name_list[i]\n",
    "            if word in mapping:\n",
    "                end_of_street_name = i\n",
    "                name_list[i] = mapping[word]  \n",
    "                \n",
    "        name_list = name_list[:(end_of_street_name+1)]\n",
    "        corrected_name = ' '.join(name_list)\n",
    "        return corrected_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's now try the function on a small sample once to see if our functions are working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alvarado-Niles Road => Alvarado Niles Road\n",
      "Alvarado-Niles Road => Alvarado Niles Road\n",
      "Avenue D => Avenue D Avenue\n",
      "Avenue D => Avenue D Avenue\n",
      "Grand Avenue #105 => Grand Avenue\n",
      "Grand Avenue #105 => Grand Avenue\n",
      "Kearny St => Kearny Street\n",
      "Kearny St => Kearny Street\n",
      "Tehama Ave => Tehama Avenue\n",
      "Tehama Ave => Tehama Avenue\n",
      "Tehama Ave => Tehama Avenue\n",
      "Tehama Ave => Tehama Avenue\n",
      "Alvarado-Niles Road => Alvarado Niles Road\n",
      "Avenue D => Avenue D Avenue\n",
      "Grand Avenue #105 => Grand Avenue\n",
      "Kearny St => Kearny Street\n",
      "Tehama Ave => Tehama Avenue\n",
      "Tehama Ave => Tehama Avenue\n"
     ]
    }
   ],
   "source": [
    "for _, element in ET.iterparse('sample_san_francisco_100.osm'):\n",
    "    for tag in element.iter(\"tag\"):\n",
    "        if tag.attrib['k'] == \"addr:street\":\n",
    "            if tag.attrib['v'] <> update_name(tag.attrib['v'], mapping):\n",
    "                print tag.attrib['v'], \"=>\", update_name(tag.attrib['v'], mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem to be working as intended. As next, let's use a sample of the map and investigate if all of the zip codes are entered properly and accurately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('94122', 46),\n",
      " ('94611', 34),\n",
      " ('94116', 20),\n",
      " ('94117', 13),\n",
      " ('94610', 11),\n",
      " ('94133', 9),\n",
      " ('94118', 8),\n",
      " ('94103', 7),\n",
      " ('94127', 6),\n",
      " ('94587', 5),\n",
      " ('94109', 4),\n",
      " ('94061', 4),\n",
      " ('94030', 3),\n",
      " ('94121', 3),\n",
      " ('94102', 3),\n",
      " ('94134', 2),\n",
      " ('94706', 2),\n",
      " ('94704', 2),\n",
      " ('94019', 2),\n",
      " ('94114', 2),\n",
      " ('94115', 2),\n",
      " ('94080', 2),\n",
      " ('94108', 2),\n",
      " ('94063', 2),\n",
      " ('94501', 1),\n",
      " ('94621', 1),\n",
      " ('94131', 1),\n",
      " ('94015', 1),\n",
      " ('94709', 1),\n",
      " ('94703', 1),\n",
      " ('94560', 1),\n",
      " ('94112', 1),\n",
      " ('94110', 1),\n",
      " ('94578', 1),\n",
      " ('94606', 1),\n",
      " ('94002', 1),\n",
      " ('94027', 1),\n",
      " ('94710', 1),\n",
      " ('94577', 1),\n",
      " ('94804', 1),\n",
      " ('94104', 1),\n",
      " ('94608', 1),\n",
      " ('94132', 1),\n",
      " ('94965', 1)]\n"
     ]
    }
   ],
   "source": [
    "def show_zipcodes(file_name):\n",
    "        num_tags = {}\n",
    "        for event, elem in ET.iterparse(file_name):\n",
    "            if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "                for tag in elem.iter(\"tag\"):\n",
    "                    if tag.attrib['k'] == \"addr:postcode\":\n",
    "                        if tag.attrib['v'] in num_tags:\n",
    "                            num_tags[tag.attrib['v']] += 1\n",
    "                        else:\n",
    "                            num_tags[tag.attrib['v']] = 1\n",
    "        return sorted(num_tags.items(), key = lambda x: x[1], reverse=True) \n",
    "                    \n",
    "    \n",
    "pprint.pprint(show_zipcodes('sample_san_francisco_100.osm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample dataset, the zipcodes are okay. However, in the full dataset, there are some wrong ones, that we need to address. \n",
    "- **CA abbreviation**: Regular zip codes should consist of 5 digits. There are some with California abbreviation. We need to get rid of these.\n",
    "- **More than 5 digits**: There are some, with more than 5 digits. \n",
    "- **First two digits**: All San Francisco zip codes should start with 94. However, there are some which start with other digits probably due to a typo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_zip(zipcode):\n",
    "    if 'CA' in re.findall('[a-zA-Z]*', zipcode) or 'ca' in re.findall('[a-zA-Z]*', zipcode):\n",
    "        if re.findall('[0-9]+', zipcode):\n",
    "            if re.findall('[0-9]+', zipcode)[0][0:2] == '94' and len(re.findall('[0-9]+', zipcode)[0]) == 5:\n",
    "                return re.findall('[0-9]+', zipcode)[0]\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        if re.findall('[0-9]+', zipcode):\n",
    "            if re.findall('[0-9]+', zipcode)[0][0:2] == '94' and len(re.findall('[0-9]+', zipcode)[0]) == 5:\n",
    "                return re.findall('[0-9]+', zipcode)[0]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the function over a small sample to see if our function is working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('95430', '=>', None)\n",
      "('95430', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95430', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n",
      "('95498', '=>', None)\n"
     ]
    }
   ],
   "source": [
    "for _, element in ET.iterparse('sample_san_francisco_10.osm'):\n",
    "    for tag in element.iter(\"tag\"):\n",
    "        if tag.attrib['k'] == \"addr:postcode\":\n",
    "            if tag.attrib['v'] <> correct_zip(tag.attrib['v']):\n",
    "                print(tag.attrib['v'], \"=>\", correct_zip(tag.attrib['v']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok the code seems to be working properly. We can use the two functions once we are reshaping the data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the data into an easy to work with python dictionary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as next, I would like to take my cleaned data and turn it into an easy to work with dictionary format, which I can later convert to JSON format. The data should have the following structure at the end:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "\"id\": \"2406124091\",\n",
    "\"type: \"node\",\n",
    "\"visible\":\"true\",\n",
    "\"created\": {\n",
    "          \"version\":\"2\",\n",
    "          \"changeset\":\"17206049\",\n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "          \"user\":\"linuxUser16\",\n",
    "          \"uid\":\"1219059\"\n",
    "        },\n",
    "\"pos\": [41.9757030, -87.6921867],\n",
    "\"address\": {\n",
    "          \"housenumber\": \"5157\",\n",
    "          \"postcode\": \"60625\",\n",
    "          \"street\": \"North Lincoln Ave\"\n",
    "        },\n",
    "\"amenity\": \"restaurant\",\n",
    "\"cuisine\": \"mexican\",\n",
    "\"name\": \"La Cabana De Don Luis\",\n",
    "\"phone\": \"1 (773)-271-5176\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, I will use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    node[\"created\"]={}\n",
    "    node[\"address\"]={}\n",
    "    node[\"pos\"]=[]\n",
    "    refs=[]\n",
    "    \n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        if \"id\" in element.attrib:\n",
    "            node[\"id\"]=element.attrib[\"id\"]\n",
    "        node[\"type\"]=element.tag\n",
    "\n",
    "        if \"visible\" in element.attrib.keys():\n",
    "            node[\"visible\"]=element.attrib[\"visible\"]\n",
    "      \n",
    "        # I will use the \"CREATED\" list as my keys to create a dict within my dict\n",
    "        for elem in CREATED:\n",
    "            if elem in element.attrib:\n",
    "                node[\"created\"][elem]=element.attrib[elem]\n",
    "                \n",
    "        # I will populate the node[\"pos\"] list with longitude and lattitude records       \n",
    "        if \"lat\" in element.attrib:\n",
    "            node[\"pos\"].append(float(element.attrib[\"lat\"])) \n",
    "        if \"lon\" in element.attrib:\n",
    "            node[\"pos\"].append(float(element.attrib[\"lon\"]))\n",
    "\n",
    "        # I will iterate through \"tag\" elements to \n",
    "        for tag in element.iter(\"tag\"):\n",
    "            if not(problemchars.search(tag.attrib['k'])):\n",
    "                if tag.attrib['k'] == \"addr:housenumber\":\n",
    "                    node[\"address\"][\"housenumber\"]=tag.attrib['v']\n",
    "                \n",
    "                # Using the function I wrote before to correct the zipcodes    \n",
    "                if tag.attrib['k'] == \"addr:postcode\":\n",
    "                    node[\"address\"][\"postcode\"]=correct_zip(tag.attrib['v'])\n",
    "                \n",
    "                # Using the function I wrote before to correct the street indicators       \n",
    "                if tag.attrib['k'] == \"addr:street\":\n",
    "                    node[\"address\"][\"street\"] = update_name(tag.attrib['v'], mapping)\n",
    "\n",
    "                if tag.attrib['k'].find(\"addr\")==-1:\n",
    "                    node[tag.attrib['k']]=tag.attrib['v']\n",
    "                    \n",
    "        for nd in element.iter(\"nd\"):\n",
    "             refs.append(nd.attrib[\"ref\"])\n",
    "                \n",
    "        if node[\"address\"] =={}:\n",
    "            node.pop(\"address\", None)\n",
    "\n",
    "        if refs != []:\n",
    "           node[\"node_refs\"]=refs\n",
    "            \n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the new dictionary format to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reshaping the dict, converting the file to JSON is fairly easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_map(file_in, pretty = False):\n",
    "    '''\n",
    "    This function process the xml openstreetmap file, \n",
    "    write a json out file and return a list of dictionaries.\n",
    "    '''\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_data = process_map('san-francisco_california.osm', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the JSON file to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As next, I will import the json file that I created to MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1048df410>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.osm\n",
    "collection = db.sf\n",
    "collection.insert_many(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), u'osm'), u'sf')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running queries and discovering the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I successfully imported our database/collection. As next, let's check the size of the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about the database:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'avgObjSize': 236.82271172110327,\n",
       " u'collections': 1,\n",
       " u'dataSize': 3486696262.0,\n",
       " u'db': u'osm',\n",
       " u'indexSize': 148475904.0,\n",
       " u'indexes': 1,\n",
       " u'numExtents': 0,\n",
       " u'objects': 14722812,\n",
       " u'ok': 1.0,\n",
       " u'storageSize': 1059987456.0,\n",
       " u'views': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Statistics about the database:')\n",
    "db.command(\"dbstats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let's see how many documents are there in our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the collection: \n",
      "14722812\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents in the collection: ') \n",
    "print(collection.find().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As next, let's discover the number of unique users as well as number of nodes and ways in our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users contributed to the map:\n",
      "2736\n",
      "Number of nodes in the map: \n",
      "13109100\n",
      "Number of ways in the map: \n",
      "1613456\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique users contributed to the map:')\n",
    "print(len(collection.distinct(\"created.user\")))\n",
    "\n",
    "print ('Number of nodes in the map: ')\n",
    "print (collection.find({\"type\":\"node\"}).count())\n",
    "\n",
    "print ('Number of ways in the map: ')\n",
    "print (collection.find({\"type\":\"way\"}).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the most contributed 20 users to the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most contributed 20 users:\n",
      "[{u'_id': u'andygol', u'count': 2995594},\n",
      " {u'_id': u'ediyes', u'count': 1776876},\n",
      " {u'_id': u'Luis36995', u'count': 1326102},\n",
      " {u'_id': u'dannykath', u'count': 1081682},\n",
      " {u'_id': u'RichRico', u'count': 808878},\n",
      " {u'_id': u'Rub21', u'count': 767318},\n",
      " {u'_id': u'calfarome', u'count': 371184},\n",
      " {u'_id': u'oldtopos', u'count': 333324},\n",
      " {u'_id': u'KindredCoda', u'count': 297044},\n",
      " {u'_id': u'karitotp', u'count': 270136},\n",
      " {u'_id': u'samely', u'count': 251124},\n",
      " {u'_id': u'abel801', u'count': 216510},\n",
      " {u'_id': u'oba510', u'count': 192422},\n",
      " {u'_id': u'DanHomerick', u'count': 183836},\n",
      " {u'_id': u'nikhilprabhakar', u'count': 174340},\n",
      " {u'_id': u'Jothirnadh', u'count': 118800},\n",
      " {u'_id': u'dchiles', u'count': 113764},\n",
      " {u'_id': u'nmixter', u'count': 113750},\n",
      " {u'_id': u'saikabhi', u'count': 113328},\n",
      " {u'_id': u'hmkandrey', u'count': 110494}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most contributed 20 users:\")\n",
    "pprint.pprint(list(collection.aggregate(\n",
    "    [{\"$group\":{\"_id\": \"$created.user\", \"count\": {\"$sum\": 1}}},\n",
    "     {\"$sort\": {\"count\": -1}},\n",
    "     {\"$limit\": 20}]                 \n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the 20 most popular cuisine sorts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular 20 cuisine sorts:\n",
      "[{u'_id': u'mexican', u'count': 464},\n",
      " {u'_id': u'chinese', u'count': 362},\n",
      " {u'_id': u'pizza', u'count': 356},\n",
      " {u'_id': u'japanese', u'count': 308},\n",
      " {u'_id': u'italian', u'count': 288},\n",
      " {u'_id': u'american', u'count': 236},\n",
      " {u'_id': u'thai', u'count': 222},\n",
      " {u'_id': u'vietnamese', u'count': 158},\n",
      " {u'_id': u'indian', u'count': 134},\n",
      " {u'_id': u'sushi', u'count': 132},\n",
      " {u'_id': u'burger', u'count': 124},\n",
      " {u'_id': u'sandwich', u'count': 110},\n",
      " {u'_id': u'asian', u'count': 108},\n",
      " {u'_id': u'seafood', u'count': 92},\n",
      " {u'_id': u'french', u'count': 64},\n",
      " {u'_id': u'mediterranean', u'count': 48},\n",
      " {u'_id': u'korean', u'count': 42},\n",
      " {u'_id': u'regional', u'count': 40},\n",
      " {u'_id': u'greek', u'count': 38},\n",
      " {u'_id': u'ice_cream', u'count': 34}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most popular 20 cuisine sorts:\")\n",
    "pprint.pprint(list(collection.aggregate(\n",
    "    [{\"$match\": {\"amenity\":{\"$exists\": 1}, \"amenity\": \"restaurant\", \"cuisine\": {\"$exists\":1}}},\n",
    "     {\"$group\": {\"_id\": \"$cuisine\", \"count\": {\"$sum\": 1}}},\n",
    "     {\"$sort\": {\"count\": -1}},\n",
    "     {\"$limit\": 20}]\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, most common 5 religion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 5 religion:\n",
      "[{u'_id': u'christian', u'count': 2008},\n",
      " {u'_id': u'buddhist', u'count': 70},\n",
      " {u'_id': u'jewish', u'count': 36},\n",
      " {u'_id': u'muslim', u'count': 16},\n",
      " {u'_id': u'unitarian_universalist', u'count': 8}]\n"
     ]
    }
   ],
   "source": [
    "print('Most common 5 religion:')\n",
    "pprint.pprint(list(collection.aggregate([\n",
    "    {\"$match\": {\"amenity\": {\"$exists\": 1}, \"amenity\": \"place_of_worship\", \"religion\": {\"$exists\":1}}},\n",
    "    {\"$group\": {\"_id\": \"$religion\", \"count\":{\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"count\": -1}},\n",
    "    {\"$limit\": 5}]\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As last, let's check 5 most popular sport types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular 5 sport sorts:\n",
      "[{u'_id': u'tennis', u'count': 1368},\n",
      " {u'_id': u'baseball', u'count': 874},\n",
      " {u'_id': u'basketball', u'count': 856},\n",
      " {u'_id': u'soccer', u'count': 300},\n",
      " {u'_id': u'swimming', u'count': 152}]\n"
     ]
    }
   ],
   "source": [
    "print('Most popular 5 sport sorts:')\n",
    "pprint.pprint(list(collection.aggregate([\n",
    "    {\"$match\": {\"sport\": {\"$exists\": 1}}},\n",
    "    {\"$group\": {\"_id\": \"$sport\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"count\": -1}},\n",
    "    {\"$limit\": 5}]\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming up with additional ideas about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The open street map dataset for San Francisco is actually pretty well structured. However, there are some flaws, which might be avoided. \n",
    "\n",
    "While controlling the `zip codes`, we saw that there were zipcodes with \"CA\" abbreviation or an additional number attached at the end. As the proper zipcodes should consist of 5 digits, it might be a good idea to make it compulsory to enter only 5 digits and nothing else. \n",
    "Similary, we saw many wrong zipcode entries such as '95121'. Knowing that the zipcodes should start with '94', the contributer could be made aware of this mistake while making the entry.\n",
    "The problem here is, that the Open Street Map is not limited to San Francisco only. That means, users can make their input on any of the cities of their choosing. In that sense, the system has to recognize the city that is being defined, check against a database for legit zipcodes and make this limitation. All in all, such system requires a lot of effort and probably is more work than, correcting the wrong zipcodes afterwards.  \n",
    "\n",
    "A similar mistake we saw was also with the `street indicators`. Many indicators were written sometimes in full form, sometimes with abbreviations. Standardizing these entries might save a great deal of time for those who would like to analyse this map. However, this might be really difficult to achieve. As an address does not always have the same components even within the same city, the developers are forced to leave this field as free text. Trying to standardize this might be really difficult and would probably not scale once we start moving across different countries with different address systems, languages and etc.    \n",
    "\n",
    "There are also several address entries that we couldn't correct programmatically(due to wrong entries, etc.) These could be identified and corrected by humans manually in order to make the map complete. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
